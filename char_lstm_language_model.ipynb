{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 30\n",
    "batch_size = 128\n",
    "train_test_split = 0.9\n",
    "nr_articles = 350  # how many articles should be loaded from the dataset\n",
    "nr_source_lines = 130000  # how many lines of source code should be loaded (python source code dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_encoded(char):\n",
    "    one_hot = torch.zeros(len(chars))\n",
    "    one_hot[chars2idx[char]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def one_hot_encoded_to_char(one_hot):\n",
    "    idx = int((one_hot == 1).nonzero()[0][0])\n",
    "    return idx2chars[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(data: str, train_test_split: float, print_summary=True):\n",
    "    train_text = in_text[:int(train_test_split*len(in_text))]\n",
    "    test_text = in_text[int(train_test_split*len(in_text)):]\n",
    "    print(f'Train size: {len(train_text)} characters')\n",
    "    print(f'Test size: {len(test_text)} characters')\n",
    "    if print_summary:\n",
    "        print(f'Read {nr_articles} articles with a total of {len(in_text)} characters.')\n",
    "    return train_text, test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### German news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = []\n",
    "with open('data/Ten_Thousand_German_News_Articles/train.csv') as in_file:\n",
    "    for line in in_file.readlines()[:nr_articles]:\n",
    "        article = list(itertools.chain(line.split(';')[1:]))\n",
    "        in_text += article\n",
    "    in_text = \"\".join(in_text)\n",
    "train_text, test_text = get_train_test_split(in_text, train_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English news dataset (Huffington Post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_data = pd.read_csv('data/english_news/articles1.csv', engine='python', error_bad_lines=False, encoding='utf-8', nrows=nr_articles)\n",
    "in_text = news_data['content'].str.cat(sep=' ')\n",
    "train_text, test_text = get_train_test_split(in_text, train_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = []\n",
    "with open('data/python_code/python.txt', encoding='utf-8') as in_file:\n",
    "    for line in in_file.readlines()[:nr_source_lines]:\n",
    "        in_text += line\n",
    "    in_text = \"\".join(in_text)\n",
    "train_text, test_text = get_train_test_split(in_text, train_test_split, print_summary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_in = train_text[:len(train_text) - (len(train_text)%seq_len)-1]\n",
    "train_text_out = train_text[1:len(train_text) - (len(train_text)%seq_len)]\n",
    "test_text_in = test_text[:len(test_text) - (len(test_text)%seq_len)-1]\n",
    "test_text_out = test_text[1:len(test_text) - (len(test_text)%seq_len)]\n",
    "print(f'len of train_text_in {len(train_text_in)}')\n",
    "print(f'len of train_text_out {len(train_text_out)}')\n",
    "print(f'len of test_text_in {len(test_text_in)}')\n",
    "print(f'len of test_text_out {len(test_text_out)}')\n",
    "chars = set(train_text + test_text)\n",
    "nr_chars = len(chars)\n",
    "print(f'nr. of unique chars: {nr_chars}')\n",
    "idx2chars = {}\n",
    "chars2idx = {}\n",
    "for i, char in enumerate(chars):\n",
    "    idx2chars[i] = char\n",
    "    chars2idx[char] = i\n",
    "\n",
    "train_text_encoded_in = torch.zeros((len(train_text_in), len(chars)))\n",
    "train_text_encoded_out = torch.zeros((len(train_text_out), len(chars)))\n",
    "test_text_encoded_in = torch.zeros((len(test_text_in), len(chars)))\n",
    "test_text_encoded_out = torch.zeros((len(test_text_out), len(chars)))\n",
    "print(train_text_encoded_in.shape)\n",
    "print(train_text_encoded_out.shape)\n",
    "print(test_text_encoded_in.shape)\n",
    "print(test_text_encoded_out.shape)\n",
    "for i, char in enumerate(train_text_in):\n",
    "    train_text_encoded_in[i][chars2idx[char]] = 1\n",
    "\n",
    "for i, char in enumerate(train_text_out):\n",
    "    train_text_encoded_out[i][chars2idx[char]] = 1\n",
    "\n",
    "for i, char in enumerate(test_text_in):\n",
    "    test_text_encoded_in[i][chars2idx[char]] = 1\n",
    "\n",
    "for i, char in enumerate(test_text_out):\n",
    "    test_text_encoded_out[i][chars2idx[char]] = 1\n",
    "\n",
    "# print(one_hot_encoded_to_char(train_text_encoded_in[0]))\n",
    "# print(one_hot_encoded_to_char(train_text_encoded_out[0]))\n",
    "# print(train_text_encoded_in[0])\n",
    "# print(one_hot_encoded_to_char(test_text_encoded_in[0]))\n",
    "# print(one_hot_encoded_to_char(test_text_encoded_out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_text[:50])\n",
    "print(test_text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_one_hot_encoded('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_text_encoded_in.shape)\n",
    "print(test_text_encoded_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nr_batches_train = int(np.floor(train_text_encoded_in.shape[0] / seq_len / batch_size))\n",
    "nr_samples_train = nr_batches_train * batch_size * seq_len\n",
    "nr_batches_test = int(np.floor(test_text_encoded_in.shape[0] / seq_len / batch_size))\n",
    "nr_samples_test = nr_batches_test * batch_size * seq_len\n",
    "\n",
    "train_text_encoded_in = train_text_encoded_in[:nr_samples_train].reshape((nr_batches_train, batch_size, seq_len, nr_chars))\n",
    "train_text_encoded_out = train_text_encoded_out[:nr_samples_train].reshape((nr_batches_train, batch_size, seq_len, nr_chars))\n",
    "test_text_encoded_in = test_text_encoded_in[:nr_samples_test].reshape((nr_batches_test, batch_size, seq_len, nr_chars))\n",
    "test_text_encoded_out = test_text_encoded_out[:nr_samples_test].reshape((nr_batches_test, batch_size, seq_len, nr_chars))\n",
    "\n",
    "print(one_hot_encoded_to_char(train_text_encoded_in[0][0][0]))\n",
    "print(one_hot_encoded_to_char(train_text_encoded_out[0][0][0]))\n",
    "print(one_hot_encoded_to_char(test_text_encoded_in[0][0][0]))\n",
    "print(one_hot_encoded_to_char(test_text_encoded_out[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_text_encoded_in.shape)\n",
    "print(test_text_encoded_in.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM input shape:** input and output tensors are provided as (batch, seq, feature)\n",
    "\n",
    "Shape: (batch, seq, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, vocab_size, batch_size):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.nb_lstm_layers = 1\n",
    "        self.batch_size = batch_size\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(input_size=vocab_size, hidden_size=hidden_dim, num_layers=1, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to character space\n",
    "        self.hidden2char = nn.Linear(hidden_dim, vocab_size, bias=True)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        lstm_out, hidden = self.lstm(sequence)\n",
    "        char_pred = self.hidden2char(lstm_out)\n",
    "        return char_pred, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, hidden_dim)\n",
    "        hidden_a = torch.randn(self.nb_lstm_layers, self.batch_size, self.hidden_dim)\n",
    "        hidden_b = torch.randn(self.nb_lstm_layers, self.batch_size, self.hidden_dim)\n",
    "\n",
    "        hidden_a = hidden_a.to(self.device)\n",
    "        hidden_b = hidden_b.to(self.device)\n",
    "\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model):\n",
    "    test_text = \"import numpy \"  # edit if necessary\n",
    "\n",
    "    test_text_encoded = torch.zeros((1, seq_len, len(chars)))\n",
    "    start_enumeration = max(seq_len-len(test_text), 0)\n",
    "    for i, c in enumerate(test_text, start=start_enumeration):\n",
    "        test_text_encoded[0][i][chars2idx[c]] = 1\n",
    "    \n",
    "    def get_most_probable_char_from_one_hot(one_hot):\n",
    "        idx = one_hot.argmax().item()\n",
    "        return idx2chars[idx]\n",
    "\n",
    "    nr_of_chars_to_generate = 1000\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.init_hidden()\n",
    "        test_text_encoded = test_text_encoded.to(device)\n",
    "        pred, hidden = model(test_text_encoded)\n",
    "        print(test_text, end='')\n",
    "        cur_text_encoded = test_text_encoded.to(device)\n",
    "        for i in range(nr_of_chars_to_generate):\n",
    "            pred, hidden = model(cur_text_encoded)\n",
    "            # print(test_text_encoded)\n",
    "            print(get_most_probable_char_from_one_hot(pred[0][-1]), end='')\n",
    "            pred_one_hot = torch.zeros(1, len(chars)).to(device)\n",
    "            pred_one_hot[0][pred[0][-1].argmax().item()] = 1\n",
    "            test_text_encoded[0] = torch.cat((test_text_encoded[0][1:], pred_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_dim = 768\n",
    "model = LSTMLanguageModel(hidden_dim=hidden_dim, vocab_size=len(chars), batch_size=batch_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_mem = []\n",
    "loss_per_epoch = []\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "generate_text_after_epochs = [1, 3, 5, 8, 10, 12, 15, 20, 25, 30, 35, 40]\n",
    "nr_epochs = 40\n",
    "print_loss_every_n_batches = 50\n",
    "for epoch in range(1, nr_epochs+1):\n",
    "    model.train()\n",
    "    model.init_hidden()\n",
    "    print(f'epoch {epoch}...')\n",
    "    running_loss = 0.0\n",
    "    i = 1\n",
    "    losses_cur_epoch = []\n",
    "    for sequence_in, sequence_out in zip(train_text_encoded_in, train_text_encoded_out):\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        sequence_in = sequence_in.to(device)\n",
    "        sequence_out = sequence_out.to(device)\n",
    "        char_predictions, hidden = model(sequence_in)\n",
    "        loss = 0.0\n",
    "        for batch_pred, batch_ground in zip(char_predictions.squeeze(), sequence_out.argmax(dim=2).squeeze()):\n",
    "            loss += loss_fn(batch_pred, batch_ground)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cur_loss = loss.item()/batch_size\n",
    "        loss_mem.append(cur_loss)\n",
    "        losses_cur_epoch.append(cur_loss)\n",
    "        running_loss += loss.item()\n",
    "        if i % print_loss_every_n_batches == print_loss_every_n_batches-1:    # print every n mini-batches\n",
    "            progress = 100*(i/len(train_text_encoded_in))\n",
    "            print(f'[{epoch}, {progress:.2f}%] loss: {running_loss / print_loss_every_n_batches / batch_size}')\n",
    "            running_loss = 0.0\n",
    "        i += 1\n",
    "    loss_per_epoch.append(np.average(losses_cur_epoch))\n",
    "    if epoch in generate_text_after_epochs:\n",
    "        print(f'\\n\\n text generation after epoch {epoch} \\n')\n",
    "        generate(model)\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "window_size = 50  # the bigger the window size, the smoother the loss curve\n",
    "loss_mem_np = np.copy(loss_mem[:(len(loss_mem) // window_size)*window_size])\n",
    "rest = np.copy(loss_mem[(len(loss_mem) // window_size)*window_size:])\n",
    "\n",
    "loss_mov_avg = np.zeros((len(loss_mem_np) // window_size))\n",
    "for i in range(0, len(loss_mem_np), window_size):\n",
    "    loss_mov_avg[i//window_size] = np.average(loss_mem_np[i:i+window_size])\n",
    "loss_mov_avg = np.concatenate((loss_mov_avg, [np.average(rest)]))\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(loss_mov_avg);\n",
    "print(f'losses per epoch: {loss_per_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
